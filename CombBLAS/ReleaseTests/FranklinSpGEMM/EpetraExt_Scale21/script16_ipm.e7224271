Warning: no access to tty (Bad file descriptor).
Thus no job control in this shell.
A has 16334935 nonzeros
B has 16335787 nonzeros
C has 123910172 nonzeros
EpetraExt multiplications finished
8.211415 seconds elapsed per iteration
C has 123910172 nonzeros
##IPMv0.983####################################################################
# 
# command : ./epetra_mult /scratch/scratchdirs/abuluc/SCALE21-RMAT/input1_0 /scratch/scratchdirs/abuluc/SCALE21-RMAT/input2_0  (completed)
# host    : nid08419/x86_64_Linux          mpi_tasks : 16 on 4 nodes
# start   : 06/26/11/20:01:35              wallclock : 194.305218 sec
# stop    : 06/26/11/20:04:49              %comm     : 22.78 
# gbytes  : 1.68542e+01 total              gflop/sec : 1.56023e-02 total
#
##############################################################################
# region  : *       [ntasks] =     16
#
#                           [total]         <avg>           min           max 
# entries                         16             1             1             1
# wallclock                  3108.86       194.304       194.301       194.305
# user                       3040.08       190.005       188.312       193.392
# system                     49.6591       3.10369       1.71611       4.14426
# mpi                        708.306       44.2691        8.6175       50.9734
# %comm                                    22.7833       4.43503       26.2338
# gflop/sec                0.0156023   0.000975144   0.000966841   0.000986815
# gbytes                     16.8542       1.05339       1.04473       1.06133
#
# PAPI_TOT_INS            6.3832e+12    3.9895e+11    3.9162e+11   4.15976e+11
# PAPI_FP_OPS            3.03161e+09   1.89476e+08   1.87862e+08   1.91743e+08
# PAPI_L1_DCA            2.65983e+12   1.66239e+11   1.63126e+11   1.72872e+11
# PAPI_L1_DCM            2.61598e+10   1.63498e+09    1.1955e+09   1.72891e+09
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Scatter                621.656            32         87.77        20.00
# MPI_Allreduce              47.0045          4608          6.64         1.51
# MPI_Rsend                  30.2936         17280          4.28         0.97
# MPI_Reduce_scatter         4.19801          1376          0.59         0.14
# MPI_Barrier                1.56202          4112          0.22         0.05
# MPI_Waitall                1.40785          1152          0.20         0.05
# MPI_Recv                   1.30186         23280          0.18         0.04
# MPI_Irecv                 0.707543         17280          0.10         0.02
# MPI_Send                  0.139586         23280          0.02         0.00
# MPI_Allgather            0.0298658           432          0.00         0.00
# MPI_Scan                0.00259768            32          0.00         0.00
# MPI_Comm_rank           0.00204951          3136          0.00         0.00
# MPI_Comm_size          0.000383442          1984          0.00         0.00
##############################################################################
# region  : ipm_noregion       [ntasks] =     16
#
#                           [total]         <avg>           min           max 
# entries                         48             3             3             3
# wallclock                  1794.92       112.182       112.182       112.183
# user                        1729.3       108.082       106.387       111.483
# system                     47.0189       2.93868       1.53209       3.96025
# mpi                        662.421       41.4013       5.68647       47.7093
# %comm                                     36.905       5.06891       42.5284
# gflop/sec               0.00357908   0.000223692   0.000221719   0.000226333
#
# PAPI_TOT_INS           5.28779e+12   3.30487e+11    3.2484e+11   3.46203e+11
# PAPI_FP_OPS            4.01513e+08   2.50946e+07   2.48732e+07   2.53908e+07
# PAPI_L1_DCA            2.19278e+12   1.37049e+11   1.34551e+11   1.43164e+11
# PAPI_L1_DCM            1.32611e+10   8.28818e+08   3.89497e+08   9.12422e+08
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Scatter                621.656            32         93.85        34.63
# MPI_Allreduce              32.8115          1088          4.95         1.83
# MPI_Rsend                  3.68589          2880          0.56         0.21
# MPI_Reduce_scatter         2.78152           256          0.42         0.15
# MPI_Barrier               0.710965           736          0.11         0.04
# MPI_Irecv                  0.47958          2880          0.07         0.03
# MPI_Waitall               0.139971           192          0.02         0.01
# MPI_Recv                  0.118316          4080          0.02         0.01
# MPI_Send                 0.0261522          4080          0.00         0.00
# MPI_Allgather           0.00760681           112          0.00         0.00
# MPI_Scan                0.00259768            32          0.00         0.00
# MPI_Comm_rank          0.000418245           576          0.00         0.00
# MPI_Comm_size          8.50147e-05           384          0.00         0.00
##############################################################################
# region  : MatMat       [ntasks] =     16
#
#                           [total]         <avg>           min           max 
# entries                         16             1             1             1
# wallclock                  1313.74       82.1085       82.1082       82.1089
# user                       1310.77       81.9234       81.8731       81.9731
# system                     2.64017       0.16501      0.124008      0.200012
# mpi                        45.8853       2.86783       2.30028       3.45741
# %comm                                    3.49271       2.80151       4.21076
# gflop/sec                0.0320318    0.00200199    0.00198504      0.002026
#
# PAPI_TOT_INS            1.0954e+12   6.84628e+10   6.67357e+10   7.00391e+10
# PAPI_FP_OPS             2.6301e+09   1.64381e+08   1.62989e+08   1.66353e+08
# PAPI_L1_DCA            4.67052e+11   2.91907e+10   2.85417e+10   2.98482e+10
# PAPI_L1_DCM            1.28987e+10   8.06167e+08   7.95514e+08   8.18491e+08
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Rsend                  26.6077         14400         57.99         2.03
# MPI_Allreduce               14.193          3520         30.93         1.08
# MPI_Reduce_scatter         1.41649          1120          3.09         0.11
# MPI_Waitall                1.26788           960          2.76         0.10
# MPI_Recv                   1.18355         19200          2.58         0.09
# MPI_Barrier               0.851059          3376          1.85         0.06
# MPI_Irecv                 0.227963         14400          0.50         0.02
# MPI_Send                  0.113434         19200          0.25         0.01
# MPI_Allgather             0.022259           320          0.05         0.00
# MPI_Comm_rank           0.00163126          2560          0.00         0.00
# MPI_Comm_size          0.000298427          1600          0.00         0.00
###############################################################################
Application 11520831 resources: utime 0, stime 0

 + --------------------------------------------------------------------------
 +        Job name: script16_ipm
 +          Job Id: 7224271.nid00003
 +          System: franklin
 +     Queued Time: Sun Jun 26 18:00:59 2011
 +      Start Time: Sun Jun 26 20:01:24 2011
 + Completion Time: Sun Jun 26 20:04:42 2011
 +            User: abuluc
 +        MOM Host: nid00579
 +           Queue: debug
 +  Req. Resources: other=QSUBPID:24245:nid00092,walltime=00:30:00
 +  Used Resources: cput=00:00:00,mem=5164kb,vmem=22308kb,walltime=00:03:18
 +     Acct String: 0
 +   PBS_O_WORKDIR: /global/homes/a/abuluc/kdt/trunk/CombBLAS/ReleaseTests/EpetraExt
 +     Submit Args: script16_ipm
 + --------------------------------------------------------------------------

