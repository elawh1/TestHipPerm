Warning: no access to tty (Bad file descriptor).
Thus no job control in this shell.
A has 16334935 nonzeros
PA has 16334935 nonzeros
PAQ has 2314681 nonzeros
EpetraExt permutations finished
4.859767 seconds elapsed per iteration
PAQ has 2314681 nonzeros
##IPMv0.983####################################################################
# 
# command : ./epetra_perm /scratch/scratchdirs/abuluc/SCALE21-RMAT/input1_0  (completed)
# host    : nid00729/x86_64_Linux          mpi_tasks : 150 on 38 nodes
# start   : 09/06/11/21:53:57              wallclock : 98.901438 sec
# stop    : 09/06/11/21:55:36              %comm     : 74.33 
# gbytes  : 4.03020e+01 total              gflop/sec : 3.04995e-03 total
#
##############################################################################
# region  : *       [ntasks] =    150
#
#                           [total]         <avg>           min           max 
# entries                        150             1             1             1
# wallclock                  14834.8       98.8984       98.8947       98.9014
# user                       14338.2       95.5877       93.1178       98.5062
# system                     249.112       1.66074       0.64404       2.14013
# mpi                        11026.3       73.5087       22.7157       79.7925
# %comm                                    74.3253       22.9696       80.6842
# gflop/sec               0.00304995    2.0333e-05   5.68778e-07   0.000313193
# gbytes                      40.302       0.26868      0.149673       1.81854
#
# PAPI_TOT_INS           5.33232e+13   3.55488e+11   2.30147e+11   3.82662e+11
# PAPI_FP_OPS            3.01644e+08   2.01096e+06         56253   3.09752e+07
# PAPI_L1_DCA            2.16817e+13   1.44545e+11   9.81418e+10   1.54423e+11
# PAPI_L1_DCM            7.07585e+10   4.71723e+08   3.28638e+08   1.16341e+09
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Allreduce              5514.46         71700         50.01        37.17
# MPI_Scatter                2976.56           150         27.00        20.06
# MPI_Reduce_scatter         1840.82         30150         16.69        12.41
# MPI_Recv                    267.48   2.61525e+06          2.43         1.80
# MPI_Barrier                170.213         90600          1.54         1.15
# MPI_Rsend                  103.661   2.21753e+06          0.94         0.70
# MPI_Bcast                  63.4463           300          0.58         0.43
# MPI_Waitall                44.5871         25791          0.40         0.30
# MPI_Send                   36.9359   2.61525e+06          0.33         0.25
# MPI_Irecv                   6.7353   2.21753e+06          0.06         0.05
# MPI_Allgather              1.35521          5400          0.01         0.01
# MPI_Scan                  0.025187           150          0.00         0.00
# MPI_Comm_rank            0.0203127         70500          0.00         0.00
# MPI_Comm_size           0.00827469         42150          0.00         0.00
##############################################################################
# region  : ipm_noregion       [ntasks] =    150
#
#                           [total]         <avg>           min           max 
# entries                        450             3             3             3
# wallclock                  7544.73       50.2982       50.2947       50.2994
# user                       7142.94       47.6196       45.2348       50.4031
# system                     195.764       1.30509      0.580037       1.91612
# mpi                        4231.34       28.2089       10.8228       31.3546
# %comm                                    56.0821       21.5167       62.3418
# gflop/sec              0.000638919   4.25946e-06   1.30181e-07   6.59477e-05
#
# PAPI_TOT_INS            2.7304e+13   1.82027e+11   1.65816e+11   1.95911e+11
# PAPI_FP_OPS            3.21372e+07        214247          6548   3.31712e+06
# PAPI_L1_DCA            1.10446e+13   7.36307e+10   6.73701e+10   7.92011e+10
# PAPI_L1_DCM            3.21316e+10    2.1421e+08   8.71565e+07   5.44912e+08
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Scatter                2976.56           150         70.35        39.45
# MPI_Allreduce              925.727          8700         21.88        12.27
# MPI_Reduce_scatter         200.454          3150          4.74         2.66
# MPI_Bcast                  63.4463           300          1.50         0.84
# MPI_Recv                   24.6789        287070          0.58         0.33
# MPI_Barrier                 19.326          9450          0.46         0.26
# MPI_Rsend                  11.2956        235194          0.27         0.15
# MPI_Waitall                4.29749          2601          0.10         0.06
# MPI_Send                   4.07946        287070          0.10         0.05
# MPI_Allgather             0.744196           900          0.02         0.01
# MPI_Irecv                 0.702854        235194          0.02         0.01
# MPI_Scan                  0.025187           150          0.00         0.00
# MPI_Comm_rank           0.00269004          7500          0.00         0.00
# MPI_Comm_size           0.00154369          4650          0.00         0.00
##############################################################################
# region  : MatMat       [ntasks] =    150
#
#                           [total]         <avg>           min           max 
# entries                        150             1             1             1
# wallclock                  7289.21       48.5948       48.5941       48.5954
# user                       7195.21       47.9681        47.635        48.579
# system                     53.3433      0.355622      0.016001      0.872054
# mpi                        6794.97       45.2998       1.73522       48.4394
# %comm                                    93.2182       3.57079       99.6795
# gflop/sec               0.00554593   3.69728e-05   1.02283e-06    0.00056915
#
# PAPI_TOT_INS           2.60192e+13   1.73461e+11   6.43315e+10   1.88022e+11
# PAPI_FP_OPS            2.69507e+08   1.79671e+06         49705   2.76581e+07
# PAPI_L1_DCA            1.06371e+13   7.09142e+10   3.00203e+10   7.56715e+10
# PAPI_L1_DCM            3.86269e+10   2.57513e+08   2.18371e+08   6.19131e+08
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Allreduce              4588.73         63000         67.53        62.95
# MPI_Reduce_scatter         1640.37         27000         24.14        22.50
# MPI_Recv                   242.801   2.32818e+06          3.57         3.33
# MPI_Barrier                150.887         81150          2.22         2.07
# MPI_Rsend                  92.3654   1.98234e+06          1.36         1.27
# MPI_Waitall                40.2896         23190          0.59         0.55
# MPI_Send                   32.8565   2.32818e+06          0.48         0.45
# MPI_Irecv                  6.03245   1.98234e+06          0.09         0.08
# MPI_Allgather             0.611011          4500          0.01         0.01
# MPI_Comm_rank            0.0176227         63000          0.00         0.00
# MPI_Comm_size             0.006731         37500          0.00         0.00
###############################################################################
Application 18914374 resources: utime 0, stime 0

 + --------------------------------------------------------------------------
 +        Job name: script150_ipm
 +          Job Id: 7324515.nid00003
 +          System: franklin
 +     Queued Time: Tue Sep  6 21:45:58 2011
 +      Start Time: Tue Sep  6 21:53:23 2011
 + Completion Time: Tue Sep  6 21:55:06 2011
 +            User: abuluc
 +        MOM Host: nid00339
 +           Queue: debug
 +  Req. Resources: other=QSUBPID:37273:nid00008,walltime=00:30:00
 +  Used Resources: cput=00:00:00,mem=5640kb,vmem=22696kb,walltime=00:01:43
 +     Acct String: m888
 +   PBS_O_WORKDIR: /global/homes/a/abuluc/kdt/trunk/CombBLAS/ReleaseTests/EpetraExt
 +     Submit Args: script150_ipm
 + --------------------------------------------------------------------------

